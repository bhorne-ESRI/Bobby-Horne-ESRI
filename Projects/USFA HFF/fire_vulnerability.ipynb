{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: fire_vulnerability\n",
    "# Purpose: Notebook to process customer CSV data containing fire locations by FIPS code, \n",
    "#          merge with other data\n",
    "# Requirements: Python3.x, ArcGIS Notebooks\n",
    "# Author(s): Jeffrey Holycross, Esri\n",
    "# Last Update: March 2023\n",
    "# Copyright: Esri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USER-DEFINED VARIABLES\n",
    "## Define the ArcGIS Online Item for the Home Fire Fatalities incidents.csv. \n",
    "## Once set, this would only change if the Portal Item was republished.\n",
    "csv_id = ''\n",
    "## Define the ArcGIS Online Item for \"SVI2020_US_county_Summarize_HFF\". \n",
    "## Once set, this would only change if the Portal Item was republished.\n",
    "hfs_id = ''\n",
    "## Define the ArcGIS Online Item ID for the CDC\\ATSDR SVI.\n",
    "## Once set, this would only change if the Portal Item was republished.\n",
    "## Not currently used\n",
    "## svi_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once variables above (`csv_id`, `hfs_id`) are defined, all cells can be run as-is using Cell --> Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard library imports\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "## Standard library \"from\" imports\n",
    "## External library imports\n",
    "import pandas as pd\n",
    "## External library \"from\" imports\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "from arcgis.gis import GIS\n",
    "## Project specific imports\n",
    "## Project specific \"from\" imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_timeouts(f):\n",
    "    \"\"\"\n",
    "    Helper function to wrap REST API networking calls in retry logic.\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    function            Required function to be executed\n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        Returns the return from the function if present, otherwise True in a successful execution\n",
    "    \"\"\"\n",
    "    for n in range(1,4):\n",
    "        try:\n",
    "            res = f()\n",
    "            ## return True if no Exception\n",
    "            return res if ('res' in locals() and res is not None) else True\n",
    "        except Exception as e:\n",
    "            if 'Column names in each table must be unique' in e.args[0]:\n",
    "                ## HFS thinks the column is already there, but res won't exist because Exception\n",
    "                return True\n",
    "            elif (\n",
    "                    (hasattr(e, 'message') and 'Your request has timed out' in e.message) or \n",
    "                    ('Your request has timed out' in e.args[0]) or\n",
    "                    ('The wait operation timed out' in e.args[0])\n",
    "            ):\n",
    "                if n < 3:\n",
    "                    print(f\"Timeout on attempt #{n}\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise RuntimeError(\"Stopping due to timeout after 3 attempts\") from None\n",
    "                    raise\n",
    "            elif ('Invalid URL' in e.args[0]):\n",
    "                if n < 3:\n",
    "                    print(f\"Invalid URL on attempt #{n}, retrying\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise RuntimeError(\"Stopping due to Invalid URL after 3 attempts\") from None\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_edit_features(a_fset, e_fset, d_fset, batch_size):\n",
    "    \"\"\"\n",
    "    Helper function to take lists of features to be added, edited, or deleted (such as from FeatureSet.features)\n",
    "    and split into chunks of size batch_size.\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    feature_list        Required list. Features to be chunked.\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    batch_size          Required integer. Size of chunks.\n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        Yields a list of features of `batch_size` size.\n",
    "    \"\"\"\n",
    "    total_features = len(a_fset) + len(e_fset) + len(d_fset)\n",
    "    num_sets = math.ceil(total_features / batch_size)\n",
    "    a_pos = 0\n",
    "    e_pos = 0\n",
    "    d_pos = 0\n",
    "    for i in range(0, total_features, batch_size):\n",
    "        ## Start with adds\n",
    "        if a_pos < len(a_fset):\n",
    "            ## Start at previous position\n",
    "            add_start = a_pos\n",
    "            ## End at position + batch_size\n",
    "            add_end = min(a_pos + batch_size,len(a_fset))\n",
    "            ## Set position to the last item included\n",
    "            a_pos = add_end\n",
    "            ## How many items included?\n",
    "            a_len = a_pos - add_start\n",
    "        else:\n",
    "            add_start = add_end = a_len = 0\n",
    "        if a_len < batch_size:\n",
    "            upd_start = e_pos\n",
    "            upd_end = min(e_pos + batch_size - a_len,len(e_fset))\n",
    "            e_pos = upd_end\n",
    "            e_len = e_pos - upd_start\n",
    "        else:\n",
    "            upd_start = upd_end = e_len = 0\n",
    "        if (a_len + e_len) < batch_size:\n",
    "            del_start = d_pos\n",
    "            del_end = min(d_pos + batch_size - (a_len + e_len),len(d_fset))\n",
    "            d_pos = del_end\n",
    "        else:\n",
    "            del_start = del_end = e_len = 0\n",
    "        yield num_sets, a_fset[add_start:add_end], e_fset[upd_start:upd_end], d_fset[del_start:del_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dates_to_utc(input_sdf,date_columns):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame and converts a list of specified columns into UTC time.\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    input_sdf           Required DataFrame. The DataFrame containing the data to convert.\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    date_columns        Required list. List of columns containing dates to convert to UTC.\n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        DataFrame (Pandas or Spatially-enabled)\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        new_time = pd.to_datetime(input_sdf[col], errors='raise', utc=True, infer_datetime_format=True)\n",
    "        input_sdf[col] = new_time\n",
    "    return input_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_features_from_sdf(add_sdf, upd_sdf, del_sdf,layer_or_table,batch_size,date_columns=None):\n",
    "    \"\"\"\n",
    "    Applies additions, edits, and deletions to a specified layer or table, using values from a Spatially Enabled DataFrame.\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    input_sdf           Required Spatially Enabled DataFrame.  The Spatially Enabled DataFrame\n",
    "                        containing the data to publish.\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    layer_or_table      Required FeatureLayer or Table. Input is assumed to be from \n",
    "                        `gis.content.get().layers[x]` or `gis.content.get().tables[x]`.\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    batch_size          Required Integer. The chunk size to send to edit_features. Esri\n",
    "                        documentation recommends <=250. In practice this will depend on the data\n",
    "                        being sent.\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    date_columns        Optional list of one of more DataFrame column names that contain Datetimes.\n",
    "                        These columns will be converted to UTC before sent to edit_features. \n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if date_columns is not None:\n",
    "        add_sdf = update_dates_to_utc(add_sdf,date_columns)\n",
    "        upd_sdf = update_dates_to_utc(upd_sdf,date_columns)\n",
    "    if len(add_sdf) > 0:\n",
    "        add_fset = add_sdf.spatial.to_featureset().features\n",
    "    else:\n",
    "        add_fset = []\n",
    "    if len(upd_sdf) > 0:\n",
    "        upd_fset = upd_sdf.spatial.to_featureset().features\n",
    "    else:\n",
    "        upd_fset = []\n",
    "    del_fset = list(del_sdf)\n",
    "    # Original feature count\n",
    "    orig_count = handle_timeouts(lambda: layer_or_table.query(return_count_only=True))\n",
    "    # Update Feature Layer\n",
    "    for i,(num_sets,adds,upds,dels) in enumerate(batch_edit_features(add_fset, upd_fset, del_fset, batch_size),1):\n",
    "        start_time = time.time()\n",
    "        res = handle_timeouts(lambda: layer_or_table.edit_features(adds=adds,updates=upds,deletes=dels,rollback_on_failure=True))\n",
    "        add_num = len([j for j in res['addResults'] if j['success']])\n",
    "        upd_num = len([j for j in res['updateResults'] if j['success']])\n",
    "        del_num = len([j for j in res['deleteResults'] if j['success']])\n",
    "        total_success = add_num + upd_num + del_num\n",
    "        print(f\"Batch {i} of {num_sets}: Added {add_num}/{len(adds)}, edited {upd_num}/{len(upds)}, deleted {del_num}/{len(dels)} in {round(time.time()-start_time,0)} seconds\")\n",
    "        if total_success != (len(adds)+len(upds)+len(dels)):\n",
    "            ## Not every edit was successful\n",
    "            errors = [k['error']['description'] for k in res['addResults'] if not k['success']]\n",
    "            errors += [k['error']['description'] for k in res['updateResults'] if not k['success']]\n",
    "            errors += [k['error']['description'] for k in res['deleteResults'] if not k['success']]\n",
    "            ## zip() is a zip generator. Convert it into a key-value dict.\n",
    "            ## Then convert the items to a list of tuples, then sort based on the second value.\n",
    "            error_counts = sorted(list(dict(zip(errors,[errors.count(e) for e in errors])).items()),key=lambda l:l[1],reverse=True)\n",
    "            print(f\"\\tERROR: Batch {i} of {num_sets} had errors:\")\n",
    "            for e in error_counts: print(f\"\\t\\t{e[1]}: {e[0]}\")\n",
    "    ## Wait 5 seconds to allow any final processing to complete\n",
    "    time.sleep(5)\n",
    "    ## Verify that final count matches expected\n",
    "    final_count = handle_timeouts(lambda: layer_or_table.query(return_count_only=True))\n",
    "    ## Expected count = Original Count + Adds - Deletes\n",
    "    expected_count = orig_count + len(add_sdf) - len(del_sdf)\n",
    "    if final_count == expected_count:\n",
    "        print(\"Processing complete! Final count matches expected number of records.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Processing is complete but final record count {final_count} does not match the expected value {expected_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fatalities_csv(gis):\n",
    "    \"\"\"\n",
    "    Function to perform business logic-specific processing\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    gis                 Required GIS object\n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        The processed DataFrame\n",
    "    \"\"\"\n",
    "    ## Retrieve existing incidents.csv\n",
    "    fp = gis.content.get(csv_id).download()\n",
    "    ## Hard code FIPS column to string to ensure that we don't lose leading zeros\n",
    "    hff_df = pd.read_csv(fp,dtype={'FIPS':str})\n",
    "    ## Remove the download\n",
    "    os.remove(fp)\n",
    "    ## Ensure that all FIPS codes are 5-digits with leading zeros\n",
    "    hff_df['FIPS'] = hff_df['FIPS'].apply(lambda f: str(f).strip().zfill(5))\n",
    "    ## For every row in the 'Date' column, apply a conversion to a datetime, and grab the year.\n",
    "    ## Create a new column with this result\n",
    "    hff_df['Year'] = hff_df['Date'].apply(lambda dt:str(pd.to_datetime(dt).year))\n",
    "    ## Create a cross tab of FIPS & fatalities by year\n",
    "    fatalities = pd.crosstab(index=hff_df['FIPS'],columns=hff_df['Year'])\n",
    "    ## Change the name of the fatalities columns so they match Fatalities_XXXX convention\n",
    "    fatalities.columns = [f'Fatalities_{y}' for y in fatalities.columns]\n",
    "    ## Make the FIPS index a column rather than index\n",
    "    return fatalities.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_field(to_layer,field_name,field_type):\n",
    "    \"\"\"\n",
    "    Function to add a new field to an existing FeatureLayer\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    to_layer            Required FeatureLayer object to add the field to\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    field_name          Required String for the field name & alias\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    field_type          Required esri Field type definition for the field being added\n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        Return from Layer.manager.add_to_definition (if any), True otherwise\n",
    "    \"\"\"\n",
    "    new_field = {\n",
    "      \"name\": field_name, \n",
    "      \"type\": field_type, \n",
    "      \"alias\": field_name, \n",
    "      \"nullable\": True, \n",
    "      \"editable\": True, \n",
    "      \"defaultValue\": None,\n",
    "      \"visible\": True\n",
    "    }\n",
    "\n",
    "    update_dict = {\"fields\": [new_field]}\n",
    "    res = handle_timeouts(lambda: to_layer.manager.add_to_definition(update_dict))\n",
    "    ## Sleep so added field can process\n",
    "    time.sleep(5)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_new_and_old_data(hfs,new_data,columns_to_update):\n",
    "    \"\"\"\n",
    "    Function to take new and old data, process as necessary and perform merging\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    gis                 Required GIS object\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    new_data            Required DataFrame with new data\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    columns_to_update   Required list of columns being updated by new_data\n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        The merged Spatially-enabled DataFrame.\n",
    "    \"\"\"\n",
    "    ## Pull existing data, including SVI fields\n",
    "    old_sdf = handle_timeouts(lambda: hfs.layers[0].query(return_geometry=False).sdf)\n",
    "    ## Reset new_data to have all of the potential FIPS values from existing SDF\n",
    "    ## Use old_sdf[['FIPS']] as left_df so that we get the index from old_sdf\n",
    "    fatalities = old_sdf[['FIPS']].merge(right=new_data,on='FIPS',how='outer')\n",
    "    ## Maximum of 3,143 FIPS values as of 2023\n",
    "    if len(fatalities) != 3143:\n",
    "        if len(old_sdf) != 3143:\n",
    "            raise RuntimeError(f\"Existing data has more FIPS codes than expected. Please manually review existing layer for duplicates or invalid information.\")\n",
    "        else:\n",
    "            ## Likely that an invalid FIPS was provided in new_data\n",
    "            ## Check for FIPS codes in new_data that are not in old_sdf FIPS column\n",
    "            invalid_fips = new_data['FIPS'].loc[~new_data['FIPS'].isin(old_sdf['FIPS'])].values.tolist()\n",
    "            raise RuntimeError(f\"Unexpected result from merging old data with new data. \" + \n",
    "                               f\"Combined FIPS count {len(fatalities)} rather than expected 3,143.\\n\" +\n",
    "                               f\"Unexpected FIPS value(s): {invalid_fips}\")\n",
    "    ## Reset Fatalities count columns to INT values, fill N/As with 0\n",
    "    for c in fatalities.columns:\n",
    "        if not c == 'FIPS':\n",
    "            fatalities[c] = fatalities[c].fillna(0).astype(int)\n",
    "\n",
    "    ## Processing\n",
    "    ## Make new_sdf a copy of the existing SDF\n",
    "    new_sdf = old_sdf.copy(True)\n",
    "    ## Overwrite the updated column(s) in new_sdf\n",
    "    for column in columns_to_update:\n",
    "        ## Are there any columns in columns_to_update that aren't already in old_sdf?\n",
    "        if column not in new_sdf.columns:\n",
    "            column_year = int(column.split('_')[1])\n",
    "            if column_year < datetime.datetime.now().year:\n",
    "                ## Don't expect prior years to be missing\n",
    "                raise RuntimeError(f\"Field for {column} not present in Hosted Feature Service data. \\n\" +\n",
    "                                f\"Verify that hfs_id is set to the right Hosted Feature service. \\n\" +\n",
    "                                f\"If set correctly, please add an Integer field named {column} to the \" + \n",
    "                                f\"{hfs.layers[0].properties['name']} layer in {hfs.title}\")\n",
    "            else:\n",
    "                ## Auto add a field with column name\n",
    "                print(f\"Adding field {column}\")\n",
    "                add_new_field(hfs.layers[0],column,\"esriFieldTypeInteger\")\n",
    "                print(f\"Successfully added field {column}\")\n",
    "                new_sdf[column] = fatalities[column]\n",
    "        else:\n",
    "            ## Overwrite\n",
    "            new_sdf[column] = fatalities[column]\n",
    "    ## Calculate new Point_Count column from all Fatalities_2XXX columns\n",
    "    fatalities_columns = [c for c in new_sdf.columns if c.startswith('Fatalities_2')]\n",
    "    ## Generate Point_Count\n",
    "    new_sdf['Point_Count'] = new_sdf[fatalities_columns].agg(sum,axis=1)\n",
    "    ## Return the processed SeDF\n",
    "    return new_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fire_rankings(hfs,sdf):\n",
    "    \"\"\"\n",
    "    Function to calcualte Fire Vulnerabilty and other related columns\n",
    "\n",
    "    ===============     ====================================================================\n",
    "    **Argument**        **Description**\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    hfs                 Required Feature Layer (used for name & title attributes in a failure)\n",
    "    ---------------     --------------------------------------------------------------------\n",
    "    sdf                 Required Spatially Enabled DataFrame with expected columns\n",
    "    ===============     ====================================================================\n",
    "\n",
    "    :return:\n",
    "        The processed Spatially Enabled DataFrame\n",
    "    \"\"\"\n",
    "    for column in ['Fatalities_per_100000','SVI_Ranking','HFF_Ranking','Fire_Vulnerability','Fire_Vulnerability_Ranking']:\n",
    "        if column not in sdf.columns:\n",
    "            col_type = 'a Double' if column == 'Fatalities_per_100000' else 'a String' if column == 'Fire_Vulnerability_Ranking' else 'an Integer'\n",
    "            raise RuntimeError(f\"Field for {column} not present in Hosted Feature Service data. \\n\" +\n",
    "                               f\"Verify that hfs_id is set to the right Hosted Feature service. \\n\" +\n",
    "                               f\"If set correctly, please add {col_type} field named {column} to the \" + \n",
    "                               f\"{hfs.layers[0].properties['name']} layer in {hfs.title}\")\n",
    "    ## Fatalities_per_100000 (double)\n",
    "    sdf['Fatalities_per_100000'] = (sdf['Point_Count']/sdf['E_TOTPOP'])*100000\n",
    "    ## HFF_Ranking (long)\n",
    "    sdf.loc[(sdf['Fatalities_per_100000'] < 10),'HFF_Ranking'] = 0\n",
    "    sdf.loc[(sdf['Fatalities_per_100000'] >= 10) & (sdf['Fatalities_per_100000'] < 25),'HFF_Ranking'] = 1\n",
    "    sdf.loc[(sdf['Fatalities_per_100000'] >= 25) & (sdf['Fatalities_per_100000'] < 35),'HFF_Ranking'] = 2\n",
    "    sdf.loc[(sdf['Fatalities_per_100000'] >= 35),'HFF_Ranking'] = 3\n",
    "    ## SVI_Ranking (long)\n",
    "    sdf.loc[(sdf['RPL_THEMES'] <= 0.25),'SVI_Ranking'] = 0\n",
    "    sdf.loc[(sdf['RPL_THEMES'] > 0.25) & (sdf['RPL_THEMES'] <= 0.5),'SVI_Ranking'] = 1\n",
    "    sdf.loc[(sdf['RPL_THEMES'] > 0.5) & (sdf['RPL_THEMES'] <= 0.75),'SVI_Ranking'] = 2\n",
    "    sdf.loc[(sdf['RPL_THEMES'] >= 0.75),'SVI_Ranking'] = 3\n",
    "    ## Fire_Vulnerability (long)\n",
    "    sdf['Fire_Vulnerability'] = sdf['HFF_Ranking'] + sdf['SVI_Ranking']\n",
    "    ## Fire_Vulnerability_Ranking (text)\n",
    "    sdf.loc[(sdf['Fire_Vulnerability'].isin([0])),'Fire_Vulnerability_Ranking'] = 'Extremely Low'\n",
    "    sdf.loc[(sdf['Fire_Vulnerability'].isin([1,2])),'Fire_Vulnerability_Ranking'] = 'Low'\n",
    "    sdf.loc[(sdf['Fire_Vulnerability'].isin([3,4])),'Fire_Vulnerability_Ranking'] = 'Medium'\n",
    "    sdf.loc[(sdf['Fire_Vulnerability'].isin([5,6])),'Fire_Vulnerability_Ranking'] = 'High'\n",
    "    return sdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main():\n",
    "gis = GIS(\"home\")\n",
    "new_data = process_fatalities_csv(gis)\n",
    "## Keep track of the Fatalities_2XXX columns that are being updated\n",
    "columns_to_update = [c for c in new_data.columns if c.startswith('Fatalities_2')]\n",
    "svc = handle_timeouts(lambda: gis.content.get(hfs_id))\n",
    "sdf = merge_new_and_old_data(svc,new_data,columns_to_update)\n",
    "## Assume that Point_Count was updated during merge_new_and_old_data\n",
    "columns_to_update.append('Point_Count')\n",
    "## Calculate Fatalities per 100k and HFF/SVI/FV rankings\n",
    "sdf = calculate_fire_rankings(svc,sdf)\n",
    "## Assume that Fire Rankings columns were updated during calculate_fire_rankings\n",
    "columns_to_update.extend(['Fatalities_per_100000','SVI_Ranking','HFF_Ranking','Fire_Vulnerability','Fire_Vulnerability_Ranking'])\n",
    "## Overwrite with SeDF should be next, in theory\n",
    "## Only need to overwrite columns in columns_to_update, plus Object ID field is required for identifying the update row\n",
    "## Ensure that columns_to_update contains the Object ID field for HFS_ID\n",
    "columns_to_update.insert(0,svc.layers[0].properties['objectIdField'])\n",
    "## Pass only columns_to_update columns to publishing functions\n",
    "print(\"Editing features...\")\n",
    "## Can go directly to edit_features_from_sdf because we don't need to identify edits (everything is an edit)\n",
    "edit_features_from_sdf(add_sdf=[],\n",
    "                       upd_sdf=sdf[columns_to_update],\n",
    "                       del_sdf=[],\n",
    "                       layer_or_table=svc.layers[0],\n",
    "                       batch_size=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce21b655b4d6c9e397d5ad93d5666c623f49909f6d0cc2f72076dafcf1b3ecfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
